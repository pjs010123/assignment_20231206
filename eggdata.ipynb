{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66fcc94",
   "metadata": {
    "id": "2da856be",
    "outputId": "a64db24f-b42f-4618-a5ed-fb48f336a4ec"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c3d926",
   "metadata": {
    "id": "00d7404d"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2627bfa0",
   "metadata": {
    "id": "7e631562",
    "outputId": "311edaec-9caa-4ecd-a929-9ae24ec70a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed323dc8",
   "metadata": {
    "id": "e767efab"
   },
   "outputs": [],
   "source": [
    "base_dir = './01.데이터/'\n",
    "train_dir = os.path.join(base_dir, 'Training/01.원천데이터/')\n",
    "validation_dir = os.path.join(base_dir, 'Validation/01.원천데이터/')\n",
    "train_mono_dir = os.path.join(train_dir, 'TS_01. MONO')\n",
    "train_color_dir = os.path.join(train_dir, 'TS_02. COLOR')\n",
    "train_archive_dir = os.path.join(train_dir, 'TS_03. 보관기간')\n",
    "validation_mono_dir = os.path.join(validation_dir, 'VS_01. MONO')\n",
    "validation_color_dir = os.path.join(validation_dir, 'VS_02. COLOR')\n",
    "validation_archive_dir = os.path.join(validation_dir, 'VS_03. 보관기간')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7cb7c3c",
   "metadata": {
    "id": "fc59f3dd",
    "outputId": "7310873a-c2e1-4894-8f6b-0f0ac34a7a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training mono images: 55692\n"
     ]
    }
   ],
   "source": [
    "print('total training mono images:', len(os.listdir(train_mono_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de41f907",
   "metadata": {
    "id": "ff39b2c5",
    "outputId": "c791eed8-d538-44e6-9f31-3e8e173198b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training color images: 43146\n"
     ]
    }
   ],
   "source": [
    "print('total training color images:', len(os.listdir(train_color_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adeddfa3",
   "metadata": {
    "id": "d2ffefe1",
    "outputId": "0d20c7f4-822f-421c-c2e6-e9d2fff8c4c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training archive images: 23045\n"
     ]
    }
   ],
   "source": [
    "print('total training archive images:', len(os.listdir(train_archive_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60172b0",
   "metadata": {
    "id": "e2b7ddf0",
    "outputId": "883ebd14-b481-49b4-98de-5f20ada6304d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation mono images: 6961\n"
     ]
    }
   ],
   "source": [
    "print('total validation mono images:', len(os.listdir(validation_mono_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64990848",
   "metadata": {
    "id": "60fa58d8",
    "outputId": "45a31ca8-a385-42fb-939d-e12d2ff06168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation color images: 5393\n"
     ]
    }
   ],
   "source": [
    "print('total validation color images:', len(os.listdir(validation_color_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c55a73",
   "metadata": {
    "id": "4fb0936d",
    "outputId": "fa352c38-b88f-4a71-d5ad-8dafd34e383c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation archive images: 2880\n"
     ]
    }
   ],
   "source": [
    "print('total validation archive images:', len(os.listdir(validation_archive_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d5351c",
   "metadata": {
    "id": "12e9a327"
   },
   "outputs": [],
   "source": [
    "def checkImage(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7299c85d",
   "metadata": {
    "id": "c57f6694"
   },
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225] )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4cf6b30",
   "metadata": {
    "id": "ca5976b6"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root = train_dir, transform = img_transforms, is_valid_file = checkImage)\n",
    "val_data = torchvision.datasets.ImageFolder(root = validation_dir, transform = img_transforms, is_valid_file = checkImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e7ef54",
   "metadata": {
    "id": "cd634537"
   },
   "outputs": [],
   "source": [
    "#By default, PyTorch’s data loaders are set to a batch_size of 1.\n",
    "BATCH_SIZE = 64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size = BATCH_SIZE)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74382f1b",
   "metadata": {
    "id": "960fe779",
    "outputId": "fcd9e43a-aba9-4249-9f80-9f5857b68120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_data_loader))\n",
    "imgs, lbls = sample\n",
    "print(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "636a6828",
   "metadata": {
    "id": "3dd29917",
    "outputId": "30d3e6a0-2fbf-43a2-e39d-3ed357eb444e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0494, -2.0494, -2.0494,  ..., -2.0494, -2.0494, -2.0494],\n",
       "         [-2.0665, -2.0323, -1.9980,  ..., -2.0494, -2.0494, -2.0494],\n",
       "         [-2.0494, -1.9980, -1.9809,  ..., -2.0494, -2.0494, -2.0494],\n",
       "         ...,\n",
       "         [-2.0323, -1.8782, -1.8782,  ..., -2.0494, -2.0323, -2.0665],\n",
       "         [-1.9638, -1.8268, -1.8782,  ..., -2.0665, -2.0665, -2.0837],\n",
       "         [-2.0152, -1.9467, -1.9638,  ..., -2.0665, -2.0837, -2.0837]],\n",
       "\n",
       "        [[-1.9657, -1.9657, -1.9657,  ..., -1.9657, -1.9657, -1.9657],\n",
       "         [-1.9832, -1.9482, -1.9132,  ..., -1.9657, -1.9657, -1.9657],\n",
       "         [-1.9657, -1.9132, -1.8957,  ..., -1.9657, -1.9657, -1.9657],\n",
       "         ...,\n",
       "         [-1.9482, -1.7906, -1.7906,  ..., -1.9657, -1.9482, -1.9832],\n",
       "         [-1.8782, -1.7381, -1.7906,  ..., -1.9832, -1.9832, -2.0007],\n",
       "         [-1.9307, -1.8606, -1.8782,  ..., -1.9832, -2.0007, -2.0007]],\n",
       "\n",
       "        [[-1.7347, -1.7347, -1.7347,  ..., -1.7347, -1.7347, -1.7347],\n",
       "         [-1.7522, -1.7173, -1.6824,  ..., -1.7347, -1.7347, -1.7347],\n",
       "         [-1.7347, -1.6824, -1.6650,  ..., -1.7347, -1.7347, -1.7347],\n",
       "         ...,\n",
       "         [-1.7173, -1.5604, -1.5604,  ..., -1.7347, -1.7173, -1.7522],\n",
       "         [-1.6476, -1.5081, -1.5604,  ..., -1.7522, -1.7522, -1.7696],\n",
       "         [-1.6999, -1.6302, -1.6476,  ..., -1.7522, -1.7696, -1.7696]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da022dec",
   "metadata": {
    "id": "ff81b70e"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d315ed",
   "metadata": {
    "id": "11f6b07a",
    "outputId": "3afaa933-dd57-4f0d-ffec-151b95698a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNeuralNetwork(\n",
      "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size = 12288):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        pass\n",
    "\n",
    "model = MyNeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77154b8",
   "metadata": {
    "id": "6de882e2",
    "outputId": "0560f217-0738-4c23-e63b-7a2e93b975b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1bc26e7",
   "metadata": {
    "id": "3ce076f8"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27a5631",
   "metadata": {
    "id": "2f0f2bcd",
    "outputId": "ba547f1c-4f6b-44cf-f1bf-9ada657b3015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1111c27",
   "metadata": {
    "id": "7041c00e",
    "outputId": "8b40cd91-37d7-4750-d1fd-246db058e9e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(start_epochs, n_epochs, model):\n",
    "    for epoch in range(start_epochs, n_epochs + 1):\n",
    "        print(f\"epoch = {epoch}\")\n",
    "\n",
    "        pass\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "    pass\n",
    "train(0, 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "765ec782",
   "metadata": {
    "id": "f2aa0320",
    "outputId": "d27a4096-0eed-4fe0-9304-fa54035ec1ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(start_epochs, n_epochs, model):\n",
    "    for epoch in range(start_epochs, n_epochs + 1):\n",
    "\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        #Set the model in training mode\n",
    "        model.train()\n",
    "\n",
    "        print(f\"epoch = {epoch}\")\n",
    "\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "    pass\n",
    "train(0, 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfe60bbd",
   "metadata": {
    "id": "aef0a664",
    "outputId": "164213a6-78d1-43fd-bd84-d8d82d21e472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch started: \n",
      "0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, epoch = 0\n",
      "batch started: \n",
      "0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, epoch = 1\n",
      "batch started: \n",
      "0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500, 1550, 1600, 1650, 1700, 1750, 1800, 1850, 1900, epoch = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyNeuralNetwork(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(start_epochs, n_epochs, model, train_loader):\n",
    "    for epoch in range(start_epochs, n_epochs + 1):\n",
    "\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        #Set the model in training mode\n",
    "        model.train()\n",
    "\n",
    "        print(f\"batch started: \")\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #print(f\"batch_idx: {batch_idx}\")\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"{batch_idx}, \", end = \"\")\n",
    "            pass\n",
    "\n",
    "        print(f\"epoch = {epoch}\")\n",
    "\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "    pass\n",
    "train(0, 2, model, train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6bcead4",
   "metadata": {
    "id": "fc4f2028"
   },
   "outputs": [],
   "source": [
    "def train_process_batches(model, train_loader, optimizer, loss_function, verbose = True ):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    if verbose:\n",
    "        print(f\"Training data batch process: \", end = \"\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        #we need to set the gradients to zero before starting to do backpropragation\n",
    "        #because PyTorch accumulates the gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "\n",
    "        #calculate the batch loss\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        #backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        ## calculate train_loss\n",
    "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        if batch_idx % 50 == 0:\n",
    "            if verbose:\n",
    "                print(f\"\\t{batch_idx}, {train_loss}\", end = \"\\n\")\n",
    "            else:\n",
    "                print(f\"\\t{batch_idx}, \", end = \"\")\n",
    "        pass\n",
    "\n",
    "    return train_loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67d1b507",
   "metadata": {
    "id": "2c3a7ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, \n",
      "Training data batch process: \t0, 0.6143525242805481\n",
      "\t50, 0.012047462165355682\n",
      "\t100, 0.006083374377340078\n",
      "\t150, 0.0040690116584300995\n",
      "\t200, 0.003056820947676897\n",
      "\t250, 0.002447893377393484\n",
      "\t300, 0.002041266532614827\n",
      "\t350, 0.0017504877177998424\n",
      "\t400, 0.0015322223771363497\n",
      "\t450, 0.0013623526319861412\n",
      "\t500, 0.0012263888493180275\n",
      "\t550, 0.0011151012731716037\n",
      "\t600, 0.0010223309509456158\n",
      "\t650, 0.000943811028264463\n",
      "\t700, 0.0008764920639805496\n",
      "\t750, 0.000818137195892632\n",
      "\t800, 0.0007670675404369831\n",
      "\t850, 0.0007219990366138518\n",
      "\t900, 0.004823549184948206\n",
      "\t950, 0.0045699444599449635\n",
      "\t1000, 0.004341673571616411\n",
      "\t1050, 0.004135122988373041\n",
      "\t1100, 0.003947333432734013\n",
      "\t1150, 0.0037758606486022472\n",
      "\t1200, 0.0036186641082167625\n",
      "\t1250, 0.00347403297200799\n",
      "\t1300, 0.003340519033372402\n",
      "\t1350, 0.003216887591406703\n",
      "\t1400, 0.0031020808964967728\n",
      "\t1450, 0.0029951853211969137\n",
      "\t1500, 0.00289541226811707\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# return trained model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 13\u001b[0m train(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, model, train_data_loader)\n",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(start_epochs, n_epochs, model, train_loader)\u001b[0m\n\u001b[0;32m      5\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_process_batches(model, train_loader, optimizer, loss_function)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrain_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# return trained model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mtrain_process_batches\u001b[1;34m(model, train_loader, optimizer, loss_function, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#calculate the batch loss\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, target)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#backward pass: compute gradient of the loss with respect to model parameters\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "def train(start_epochs, n_epochs, model, train_loader):\n",
    "    for epoch in range(start_epochs, n_epochs + 1):\n",
    "        print(f\"Epoch: {epoch}, \", end = \"\\n\")\n",
    "       # initialize variables to monitor training and validation loss\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        #train model\n",
    "        train_loss = train_process_batches(model, train_loader, optimizer, loss_function)\n",
    "\n",
    "        print(f\"\\ntrain_loss = {train_loss}\")\n",
    "    # return trained model\n",
    "    return model\n",
    "train(0, 1, model, train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52f7f0db",
   "metadata": {
    "id": "30e58d3e"
   },
   "outputs": [],
   "source": [
    "def eval_process_batches(model, val_loader, optimizer, loss_function, verbose = True ):\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    if verbose:\n",
    "        print(f\"Test data batch process: \", end = \"\")\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        ## update the average validation loss\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # update average validation loss\n",
    "        valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            if verbose:\n",
    "                print(f\"\\t{batch_idx}, {valid_loss}\", end = \"\\n\")\n",
    "            else:\n",
    "                print(f\"\\t{batch_idx}, \", end = \"\")\n",
    "        pass\n",
    "    return valid_loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74b0a0fb",
   "metadata": {
    "id": "d9d7d1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, \n",
      "\t0, \t50, \t100, \t150, \t200, \t250, \t300, \t350, \t400, \t450, \t500, \t550, \t600, \t650, \t700, \t750, \t800, \t850, \t900, \t950, \t1000, \t1050, \t1100, \t1150, \t1200, \t1250, \t1300, \t1350, \t1400, \t1450, \t1500, "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# return trained model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 17\u001b[0m train(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, model, train_data_loader, val_data_loader)\n",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(start_epochs, n_epochs, model, train_loader, val_loader)\u001b[0m\n\u001b[0;32m      5\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_process_batches(model, train_loader, optimizer, loss_function, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m eval_process_batches(model, val_loader, optimizer, loss_function, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrain_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m, in \u001b[0;36mtrain_process_batches\u001b[1;34m(model, train_loader, optimizer, loss_function, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#calculate the batch loss\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, target)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#backward pass: compute gradient of the loss with respect to model parameters\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "def train(start_epochs, n_epochs, model, train_loader, val_loader):\n",
    "    for epoch in range(start_epochs, n_epochs+1):\n",
    "        print(f\"Epoch: {epoch}, \", end = \"\\n\")\n",
    "       # initialize variables to monitor training and validation loss\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        #train model\n",
    "        train_loss = train_process_batches(model, train_loader, optimizer, loss_function, verbose = False)\n",
    "        valid_loss = eval_process_batches(model, val_loader, optimizer, loss_function, verbose = True)\n",
    "\n",
    "\n",
    "        print(f\"\\ntrain_loss = {train_loss}\")\n",
    "        print(f\"\\nvalid_loss = {valid_loss}\")\n",
    "\n",
    "    # return trained model\n",
    "    return model\n",
    "train(0, 5, model, train_data_loader, val_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55568d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9e8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
